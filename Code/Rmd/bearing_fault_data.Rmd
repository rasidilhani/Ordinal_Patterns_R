---
title: "Bearing_Fault_Data"
output: html_document
date: "2024-07-21"
author: "Rasika and Keila"
---

```{r setup}
# Load required libraries
library(readxl)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(statcomp)
```

# Step 1: Data Collection (Done)
Download Data:
Normal Baseline Data: Download
12k Drive End Bearing Fault Data: Download
48k Drive End Bearing Fault Data: Download
Fan-End Bearing Fault Data: Download

# Step 2: Data Preparation
Read and Organize Data:
Use the R.matlab library in R to read the Matlab files.
Organize the data into a structured format (e.g., data frames or lists) for analysis.

```{r}

library(rprojroot)

# Install and load required packages if not already installed
if (!require("R.matlab")) {
  install.packages("R.matlab")
  library(R.matlab)
}

if (!require("writexl")) {
  install.packages("writexl")
  library(writexl)
}

# Function to read a .mat file and convert it to a data frame
read_mat_to_df <- function(file_path) {
  if (file.exists(file_path)) {
    data <- readMat(file_path)
    # Assuming the time series data is stored in a variable named 'X'
    # Adjust according to the actual structure of your .mat files
    data_frame <- as.data.frame(data$X)
    return(data_frame)
  } else {
    stop(paste("File not found:", file_path))
  }
}

# Define o caminho base do projeto usando rprojroot
base_path <- find_root(has_file("README.md"))

# Especifica os caminhos relativos dos arquivos .mat sem repetição do diretório
file_paths <- c(
  file.path(base_path, "Data", "mat", "Normal_Baseline_Data", "97.mat"),
  file.path(base_path, "Data", "mat", "Normal_Baseline_Data", "98.mat"),
  file.path(base_path, "Data", "mat", "Normal_Baseline_Data", "99.mat"),
  file.path(base_path, "Data", "mat", "Normal_Baseline_Data", "100.mat")
)

# Verifica se os arquivos existem
file_paths_exist <- sapply(file_paths, file.exists)
print(file_paths_exist)

if (any(!file_paths_exist)) {
  stop("Some files do not exist: ", paste(file_paths[!file_paths_exist], collapse = ", "))
}


# Read the files into data frames
df_list <- lapply(file_paths, read_mat_to_df)

# Combine data frames into a single data frame
combined_df <- do.call(rbind, df_list)

# Write the combined data frame to an XLS file
#write_xlsx(combined_df, "Normal_Baseline_Data.xlsx") #Use .csv is easier to handle large data

message("Data has been successfully written to 'Normal_Baseline_Data.xlsx'.")

```

# Step 3: Preliminary Analysis
Compute HxC Points:
Use the statcomp library to compute the HxC point for each compatible time series.
Check the documentation to match time series with the same measurement.

```{r}

```

# Step 4: Visualization
Visualize Data:
Plot the HxC points in the HxC manifold.
Identify if different categories form distinct clusters.

```{r}

```

# Step 5: Literature Review
Analyze Literature:
Analyze the references from the Web of Science and additional references from Scopus.
Identify relevant scientific questions and gaps in the literature.
Review existing studies on Ordinal Patterns or Permutation Entropy.

## Research on the Use of CWRU Bearing Data Center Data

The Case Western Reserve University (CWRU) Bearing Data Center provides a well-known dataset extensively used in research for bearing fault diagnosis and machine learning applications. 

## Key Areas of Research

### 1. Fault Diagnosis and Prognosis
Many studies have utilized the CWRU dataset to develop and validate fault diagnosis methods for bearings. Techniques such as deep learning, machine learning, and traditional signal processing have been applied to detect and classify faults in bearings https://www.mdpi.com/2076-3417/8/12/2357 and https://link.springer.com/chapter/10.1007/978-3-031-07322-9_32.

### 2. Benchmarking and Algorithm Development
The dataset is commonly used to benchmark new algorithms and methods for fault detection and classification. This includes the use of Convolutional Neural Networks (CNNs), Transfer Learning (TL), and other advanced neural network architectures to improve diagnostic accuracy and robustness under varying operational conditions https://www.mdpi.com/2076-3417/8/12/2357.

### 3. Signal Processing Techniques
Researchers have applied various signal processing techniques, such as the Stockwell Transform, to extract features from the vibration data. These features are then used for fault detection and classification, showcasing the dataset's utility in validating new signal processing methods https://www.mdpi.com/2076-3417/8/12/2357;.

### 4. Unsupervised Learning and Clustering
The dataset has also been employed in unsupervised learning approaches, where clustering algorithms are used to distinguish between different bearing states without labeled data. This helps in understanding the natural grouping of data and improving the reliability of fault diagnosis systems https://link.springer.com/chapter/10.1007/978-3-031-07322-9_32.

## Application of Ordinal Patterns and Bandt-Pompe Theory
Regarding the use of Ordinal Patterns and Bandt-Pompe Theory, there is no specific mention in the available literature that directly applies these methods to the CWRU dataset. Ordinal Patterns and the Bandt-Pompe approach are typically used in the context of time series analysis to detect complex patterns and anomalies. While these methods could theoretically be applied to the CWRU bearing data for novel insights, existing studies have primarily focused on more conventional machine learning and signal processing techniques.

The CWRU Bearing Data Center dataset continues to be a valuable resource for researchers in the field of fault diagnosis and machine learning. It supports the development and validation of new diagnostic algorithms, benchmarking of various approaches, and exploration of advanced signal processing techniques. Although the Bandt-Pompe Theory and Ordinal Patterns have not been prominently featured in this context, there remains potential for future research to explore these methodologies using the CWRU dataset.

-------------------------------------------------------------------------------------------------------
Given the existing research on the CWRU Bearing Data Center dataset and the lack of application of Bandt-Pompe Theory and Ordinal Patterns, you can contribute to the state of the art by exploring these methodologies.

## Potential Research Directions

### 1. Application of Ordinal Patterns for Feature Extraction
**Objective**: Investigate the effectiveness of ordinal patterns in extracting features from vibration signals of bearings.
**Approach**: Apply ordinal pattern analysis to the vibration data to capture the dynamics and inherent structures in the time series. Evaluate the discriminative power of these features for fault diagnosis.

### 2. Permutation Entropy for Fault Detection
**Objective**: Utilize permutation entropy as a measure to detect and classify bearing faults.
**Approach**: Compute permutation entropy for the time series data and use it as a feature for machine learning models. Compare its performance with traditional features like statistical measures and spectral features.

### 3. HxC Plane Analysis
**Objective**: Explore the use of the HxC plane (entropy-complexity plane) to distinguish between different bearing conditions.
**Approach**: Calculate the permutation entropy and statistical complexity for the time series data. Plot the results in the HxC plane and analyze the clustering of different fault types. This could provide insights into the complexity and randomness of the signal patterns associated with different fault conditions.

### 4. Integration of Ordinal Pattern and Machine Learning Models
**Objective**: Develop machine learning models that integrate ordinal pattern features for improved fault classification.
**Approach**: Extract ordinal pattern-based features from the dataset and use them to train various machine learning models (e.g., SVM, Random Forest, Neural Networks). Assess the models' performance and compare it with models using traditional features.

### 5. Multiscale Permutation Entropy
**Objective**: Investigate the use of multiscale permutation entropy for multi-scale analysis of bearing vibration signals.
**Approach**: Compute permutation entropy at multiple scales to capture the signal’s complexity at different time resolutions. This can help in understanding the multi-scale behavior of the vibration signals and its correlation with bearing conditions.

### 6. Hybrid Models Combining Ordinal Patterns and Deep Learning
**Objective**: Enhance fault diagnosis accuracy by combining ordinal pattern-based features with deep learning architectures.
**Approach**: Use ordinal patterns to preprocess the data and extract meaningful features. Feed these features into deep learning models (e.g., CNNs, LSTMs) to classify bearing faults. Evaluate the hybrid model's performance against traditional and pure deep learning approaches.

By applying Bandt-Pompe Theory and Ordinal Patterns to the CWRU Bearing Data Center dataset, you can contribute novel insights and methodologies to the field of bearing fault diagnosis. This research has the potential to enhance the understanding of complex vibration signals and improve the accuracy of fault detection systems.

For further information and to get started, you can refer to the articles and resources on platforms like ScienceDirect - https://www.sciencedirect.com/, IEEE Xplore - https://ieeexplore.ieee.org/, and SpringerLink - https://link.springer.com/. These platforms provide access to a wealth of research papers and articles that can guide your study.
-------------------------------------------------------------------------------------------------------

## Steps for Conducting the Research

### Literature Review
Conduct a thorough literature review on the application of ordinal patterns and permutation entropy in time series analysis. Identify gaps and opportunities for applying these methods to bearing fault diagnosis.

### Data Preprocessing
Clean and preprocess the CWRU dataset to ensure it is suitable for analysis. This may include normalization, noise reduction, and segmentation of the time series data.

### Feature Extraction
Implement algorithms to extract ordinal patterns, permutation entropy, and complexity measures from the vibration signals.

### Model Development
Develop and train machine learning models using the extracted features. Experiment with different algorithms and parameter settings to optimize performance.

### Evaluation
Evaluate the models using standard metrics such as accuracy, precision, recall, and F1-score. Compare the performance with baseline models using traditional features.

### Analysis and Interpretation
Analyze the results to understand the effectiveness of ordinal patterns and permutation entropy in distinguishing different bearing conditions. Provide insights into the potential advantages and limitations of these methods.
-------------------------------------------------------------------------------------------------------

# Step 6: Bibliometric Analysis
Identify Relevant Journals:
Use bibliometrix to analyze the database and identify the most relevant journals for the topic.

# Step 7: Study Design
Design the Study:
Based on the results and literature review, design a study to be submitted to a journal.
Define the research questions, hypotheses, and methodology.

# Step 8: Results Presentation
Prepare for Conference:
Prepare the findings for presentation at the NZSA conference or other.
Create visualizations and summaries of the HxC points and clusters.

# Step 9: Writing and Submission
Write and Submit Paper:
Draft the paper based on the study design and results.
Follow the guidelines of the identified relevant journals.
Submit the paper for publication.

# Tools and Resources
R.matlab Library: For reading Matlab files in R.
statcomp Library: For computing HxC points.
JabRef: For managing BibTeX files.
bibliometrix: For bibliometric analysis.

```{r}
# Function to compute ordinal patterns for a given embedding dimension
compute_ordinal_patterns <- function(series, emb_dim) {
  n <- length(series)
  patterns <- vector("list", n - emb_dim + 1)
  
  for (i in 1:(n - emb_dim + 1)) {
    subseries <- series[i:(i + emb_dim - 1)]
    patterns[[i]] <- order(subseries)
  }
  return(patterns)
}

# Function to find the frequency of each ordinal pattern
ordinal_pattern_frequencies <- function(series, emb_dim) {
  patterns <- compute_ordinal_patterns(series, emb_dim)
  patterns_str <- sapply(patterns, paste, collapse = "")
  freq_table <- table(patterns_str)
  
  # Convert to data frame for better readability
  freq_df <- as.data.frame(freq_table)
  colnames(freq_df) <- c("Pattern", "Frequency")
  return(freq_df)
}

# Function to calculate probabilities from frequencies
calculate_probabilities <- function(freq_df) {
  total_patterns <- sum(freq_df$Frequency)
  probabilities <- freq_df$Frequency / total_patterns
  freq_df$Probability <- probabilities
  return(freq_df)
}

# Function to calculate permutation entropy (Shannon entropy)
calculate_permutation_entropy <- function(probabilities) {
  entropy <- -sum(probabilities * log2(probabilities), na.rm = TRUE)
  return(entropy / log2(length(probabilities))) # Normalized
}

# Function to calculate Jensen-Shannon divergence
js_divergence <- function(P, Q) {
  M <- 0.5 * (P + Q)
  kl_divergence <- function(P, Q) {
    sum(P * log2(P / Q), na.rm = TRUE)
  }
  jsd <- 0.5 * kl_divergence(P, M) + 0.5 * kl_divergence(Q, M)
  return(jsd)
}

# Function to calculate Jensen-Shannon entropy
calculate_js_entropy <- function(probabilities) {
  uniform_prob <- rep(1 / length(probabilities), length(probabilities))
  jsd <- js_divergence(probabilities, uniform_prob)
  js_entropy <- sqrt(jsd)
  return(js_entropy)
}
```



```{r}

# Function to plot time series
plot_time_series <- function(time_series, title) {
  data <- data.frame(Time = seq_along(time_series), Value = time_series)
  ggplot(data, aes(x = Time, y = Value)) +
    geom_line() +
    ggtitle(title) +
    theme_minimal()
}

# Function to plot frequencies of the ordinal patterns (Histogram)
plot_frequencies <- function(freq_df, title_txt) {
  ggplot(freq_df, aes(x = factor(Pattern, levels = unique(Pattern)), y = Probability)) +
    geom_bar(stat = "identity", fill = "blue", color = "red", width = 0.7) +
    xlab("Pattern") +
    ylab("Probability") +
    ggtitle(title_txt) +
    theme_bw() +
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1),
      axis.text.y = element_text(size = 12),
      axis.title.y = element_text(size = 14),
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    ) +
    ylim(0, 1)
}
```

```{r}
file_paths
````



```{r}

# Instale os pacotes se ainda não estiverem instalados
if (!requireNamespace("rprojroot", quietly = TRUE)) {
  install.packages("rprojroot")
}
if (!requireNamespace("readxl", quietly = TRUE)) {
  install.packages("readxl")
}

# Carrega os pacotes necessários
library(rprojroot)
library(readxl)

# Define o caminho base do projeto usando rprojroot
base_path <- find_root(has_file("README.md"))

# Construa o caminho relativo para o arquivo Excel
file_path <- file.path(base_path, "Data", "XLS", "Normal Baseline Data.xlsx")

# Verifique se o arquivo existe
if (!file.exists(file_path)) {
  stop("O arquivo não existe: ", file_path)
}

# Define sheet names
sheet_names <- c("Normal 0", "Normal 1", "Normal 2", "Normal 3")

# Initialize lists to store time series data and results
DE_Time <- list()
FE_Time <- list()
results <- list()

# Read each sheet and extract the time series data
for (sheet in sheet_names) {
  data <- read_excel(file_path, sheet = sheet)
  
  # Print column names for debugging
  print(paste("Sheet:", sheet, "Columns:", paste(colnames(data), collapse = ", ")))
  
  if ("DE_Time" %in% colnames(data) & "FE_Time" %in% colnames(data)) {
    DE_Time[[sheet]] <- data$DE_Time
    FE_Time[[sheet]] <- data$FE_Time
    
    # Plot the time series
    print(plot_time_series(data$DE_Time, paste(sheet, "DE_Time")))
    print(plot_time_series(data$FE_Time, paste(sheet, "FE_Time")))
    
    for (emb_dim in 3:6) {
      # Calculate ordinal patterns and their frequencies
      pattern_frequencies_DE <- ordinal_pattern_frequencies(data$DE_Time, emb_dim)
      pattern_frequencies_FE <- ordinal_pattern_frequencies(data$FE_Time, emb_dim)
      
      # Calculate probabilities
      probabilities_df_DE <- calculate_probabilities(pattern_frequencies_DE)
      probabilities_df_FE <- calculate_probabilities(pattern_frequencies_FE)
      probabilities_DE <- probabilities_df_DE$Probability
      probabilities_FE <- probabilities_df_FE$Probability
      
      # Calculate normalized Shannon entropy
      shannon_entropy_DE <- calculate_permutation_entropy(probabilities_DE)
      shannon_entropy_FE <- calculate_permutation_entropy(probabilities_FE)
      
      # Calculate normalized Jensen-Shannon entropy
      js_entropy_DE <- calculate_js_entropy(probabilities_DE)
      js_entropy_FE <- calculate_js_entropy(probabilities_FE)
      
      # Store the results
      results[[paste(sheet, emb_dim, "DE_Time")]] <- list(
        Sheet = sheet,
        Emb_Dim = emb_dim,
        ShannonEntropy = shannon_entropy_DE,
        JSEntropy = js_entropy_DE
      )
      results[[paste(sheet, emb_dim, "FE_Time")]] <- list(
        Sheet = sheet,
        Emb_Dim = emb_dim,
        ShannonEntropy = shannon_entropy_FE,
        JSEntropy = js_entropy_FE
      )
      
      # Plot the frequencies
      print(plot_frequencies(probabilities_df_DE, paste(sheet, "DE_Time", "Embedding Dimension =", emb_dim)))
      print(plot_frequencies(probabilities_df_FE, paste(sheet, "FE_Time", "Embedding Dimension =", emb_dim)))
    }
  } else {
    warning(paste("Sheet", sheet, "does not contain DE_Time or FE_Time columns."))
  }
}

```


```{r}
# Prepare data for complexity plane for each embedding dimension
complexity_data <- do.call(rbind, lapply(names(results), function(name) {
  result <- results[[name]]
  data.frame(
    Sheet = result$Sheet,
    Emb_Dim = result$Emb_Dim,
    ShannonEntropy = result$ShannonEntropy,
    JSEntropy = result$JSEntropy
  )
}))

# Function to generate feasible region bounds
generate_feasible_region <- function() {
  data.frame(
    ShannonEntropy = c(0, 1, 1, 0, 0),
    JSEntropy = c(0, 0, 1, 1, 0)
  )
}

# Generate feasible region
feasible_region <- generate_feasible_region()

# Function to plot the complexity plane with feasible region for each embedding dimension
plot_complexity_planes <- function(data, feasible_region) {
  unique_dims <- unique(data$Emb_Dim)
  plots <- list()
  
  for (dim in unique_dims) {
    data_dim <- subset(data, Emb_Dim == dim)
    p <- ggplot() +
      geom_polygon(data = feasible_region, aes(x = ShannonEntropy, y = JSEntropy), fill = "lightgray", alpha = 0.5) +
      geom_point(data = data_dim, aes(x = ShannonEntropy, y = JSEntropy, color = Sheet), size = 3) +
      theme_minimal() +
      xlab("Normalized Shannon Entropy") +
      ylab("Normalized Jensen-Shannon Entropy") +
      ggtitle(paste("Complexity Plane H*C - Embedding Dimension", dim)) +
      scale_color_discrete(name = "Sheet") +
      xlim(0, 1) +
      ylim(0, 1)
    plots[[as.character(dim)]] <- p
  }
  return(plots)
}

# Convert results to a data frame for plotting
complexity_data_df <- as.data.frame(complexity_data)

# Plot the complexity planes
complexity_planes <- plot_complexity_planes(complexity_data_df, feasible_region)

# Print the complexity planes
for (plot in complexity_planes) {
  print(plot)
}

```



```{r}
# Prepare data for complexity plane with confidence interval
complexity_data <- do.call(rbind, lapply(names(results), function(name) {
  result <- results[[name]]
  data.frame(
    Sheet = result$Sheet,
    Emb_Dim = result$Emb_Dim,
    ShannonEntropy = result$ShannonEntropy,
    JSEntropy = result$JSEntropy
  )
}))

# Function to generate feasible region bounds
generate_feasible_region <- function() {
  data.frame(
    ShannonEntropy = c(0, 1, 1, 0, 0),
    JSEntropy = c(0, 0, 1, 1, 0)
  )
}

# Generate feasible region
feasible_region <- generate_feasible_region()

# Function to calculate mean and standard deviation for each time series
calculate_summary_stats <- function(data) {
  summary_stats <- data %>%
    group_by(Sheet, Emb_Dim) %>%
    summarize(
      MeanShannon = mean(ShannonEntropy),
      SdShannon = sd(ShannonEntropy),
      MeanJSEntropy = mean(JSEntropy),
      SdJSEntropy = sd(JSEntropy)
    )
  return(summary_stats)
}

# Calculate summary statistics
summary_stats <- calculate_summary_stats(complexity_data)

# Merge summary statistics with the original complexity data
complexity_data_with_stats <- merge(complexity_data, summary_stats, by = c("Sheet", "Emb_Dim"))

# Function to plot the complexity plane with confidence intervals for each embedding dimension
plot_complexity_planes <- function(data, feasible_region) {
  unique_dims <- unique(data$Emb_Dim)
  plots <- list()
  
  for (dim in unique_dims) {
    data_dim <- subset(data, Emb_Dim == dim)
    p <- ggplot() +
      geom_polygon(data = feasible_region, aes(x = ShannonEntropy, y = JSEntropy), fill = "lightgray", alpha = 0.5) +
      geom_point(data = data_dim, aes(x = ShannonEntropy, y = JSEntropy, color = Sheet), size = 3) +
      geom_errorbar(data = data_dim, aes(x = MeanShannon, ymin = MeanJSEntropy - SdJSEntropy, ymax = MeanJSEntropy + SdJSEntropy, color = Sheet), width = 0.02) +
      geom_errorbarh(data = data_dim, aes(y = MeanJSEntropy, xmin = MeanShannon - SdShannon, xmax = MeanShannon + SdShannon, color = Sheet), height = 0.02) +
      theme_minimal() +
      xlab("Normalized Shannon Entropy") +
      ylab("Normalized Jensen-Shannon Entropy") +
      ggtitle(paste("Complexity Plane H*C for Embedding Dimension", dim))
    plots[[as.character(dim)]] <- p
  }
  return(plots)
}

# Generate plots for each embedding dimension
plots <- plot_complexity_planes(complexity_data_with_stats, feasible_region)

# Display the plots
for (plot in plots) {
  print(plot)
}


```


```{r}
# Prepare data for complexity plane
complexity_data <- do.call(rbind, lapply(names(results), function(name) {
  result <- results[[name]]
  data.frame(
    Sheet = result$Sheet,
    Emb_Dim = result$Emb_Dim,
    ShannonEntropy = result$ShannonEntropy,
    JSEntropy = result$JSEntropy
  )
}))

# Function to plot the complexity plane
plot_complexity_plane <- function(data) {
  ggplot(data, aes(x = ShannonEntropy, y = JSEntropy, color = as.factor(Emb_Dim))) +
    geom_point(size = 3) +
    theme_minimal() +
    xlab("Normalized Shannon Entropy") +
    ylab("Normalized Jensen-Shannon Entropy") +
    ggtitle("Complexity Plane H*C") +
    scale_color_discrete(name = "Embedding Dimension")
}

# Convert results to a data frame for plotting
complexity_data_df <- as.data.frame(complexity_data)

# Plot the complexity plane
print(plot_complexity_plane(complexity_data_df))

# Create a summary table for each time series
summary_table <- do.call(rbind, lapply(names(results), function(name) {
  result <- results[[name]]
  data.frame(
    Sheet = result$Sheet,
    Emb_Dim = result$Emb_Dim,
    ShannonEntropy = result$ShannonEntropy,
    JSEntropy = result$JSEntropy
  )
}))

# Print the summary table
print(summary_table)

```
