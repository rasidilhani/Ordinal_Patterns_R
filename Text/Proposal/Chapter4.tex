\chapter{Future Works}\label{C:futw}

As a short summary, we have completed the necessary preliminaries studies on various topics such as:
\begin{itemize}
    \item Entropy
    \item Complexity
    \item Entropy Complexity Plane
    \item Confidence interval
    %\item Multinomial modal.
\end{itemize}

We have also examined key research articles by Bandt and Pompe \cite{PhysRevLett.88.174102}, along with an overview of the area focusing on four seminal works. These include:
\begin{itemize}
	\item López-Ruiz et al. \cite{lopez1995statistical}, who introduced the concept of statistical complexity;
	\item Lamberti et al. \cite{lamberti2004intensive}, who applied López-Ruiz's idea using the Euclidean distance;
	\item Rosso et al. \cite{EEGAnalysisUsingWaveletBasedInformationTools}, who proposed the entropy-complexity plane as a diagnostic tool; and
	\item Martin et al. \cite{Martin2006}, who defined the theoretical boundaries of this generalized statistical complexity measure.
\end{itemize}

In addition, we reviewed recent work by Rey et al. \cite{Rey2025,Rey2023a,Rey2023}, which investigates the statistical properties of entropy derived from ordinal patterns, including the asymptotic distribution under the Multinomial law and the behavior of permutation entropy. 

As a case study, we computed the Shannon entropy, statistical complexity, and their associated asymptotic variances based on the probability distribution of ordinal patterns. Using these results, we derived confidence intervals for both entropy and complexity. The analysis was further visualized using the entropy–complexity plane, offering insights into the underlying system dynamics. All computations were performed using two large-sample datasets under the asymptotic distribution, as detailed in Chapter~\ref{C:aim}, Section~\ref{Sec:CaseStudy}. 


The formulas and procedures used to analyze the case study are summarized as follows:
\begin{itemize}
	\item Calculate the Shannon entropy of the time series.
	\item Calculate the statistical complexity.
	\item Estimate the asymptotic variance for Shannon entropy.
	\item Construct confidence intervals for entropy.
	\item Plot the results in the entropy–complexity plane.
	\item Divide the data into batches (batch size = 10,000).
	\item Repeat the above calculations for each batch.
	\item Graphically represent the results of the two time series across batches in the entropy–complexity plane.
	\item Finally, the results are analyzed for time series clustering, as shown in the final output in Figure~\ref{fig:EntopyComplexity Plane}
\end{itemize}

%%% ACF Do not repeat what has already been defined.
Asymptotic distribution of normalized Shannon entropy $H(\mathbf{p})$ was derived under the assumption of independent ordinal patterns, following the Multinomial law. As a foundational step, we use the normalized Shannon entropy formula: 
\begin{equation}
	H(\mathbf{p})=-\dfrac{1}{\log k}\sum^{k}_{\ell=1}p_{\ell} \ln{p_{\ell}}.
\end{equation}
Where, $k=D!$ is the number of possible ordinal patterns. To evaluate statistical complexity, we compute the Jensen–Shannon divergence between the histogram of proportion $\mathbf{p}$ and the uniform probability function $\mathbf{u}=(1/k, 1/k, \dots, 1/k)$, defined by:  
\begin{equation}
	Q'(\mathbf{p,u})=\sum^k_{\ell=1} p_\ell\log\dfrac{p_\ell}{u_\ell}+u_\ell\log\dfrac{u_\ell}{p_\ell}.
\end{equation}

This disequilibrium measure is normalized using:
\begin{equation}
	Q=\dfrac{Q'}{\max{(Q')}},
\end{equation}
where $\max(Q')$ is defined as follows
\begin{equation}
	\max(Q')=-2 \left[\dfrac{k+1}{k}\log(k+1)-2\log(2k)+\log k\right].
\end{equation}

The statistical complexity is then calculated as:
\begin{equation}
	C=HQ,
\end{equation}
where both $H$ and $Q$ are normalized quantities, therefore $C$ is also normalized.   

Then the entropy-complexity plane, which is a two-dimensional representation used to graphically represent the results. 

As a key component of our research, we also calculated the asymptotic variance of the Shannon entropy estimator. The estimated normalized entropy based on sample proportions $\widehat{\bm{p}}$ is: 
\begin{equation}
	H_s(\widehat{\bm{p}})=-\dfrac{1}{\log k}\sum_{\ell=1}^{k}\widehat{p_\ell}\log\widehat{p_\ell}.
\end{equation}

The corresponding asymptotic variance under the Multinomial model is given by:
\begin{equation}
	\widehat{\sigma}^2_p=\dfrac{1}{n}\sum_{\ell=1}^{k}p_\ell(1-p_\ell)(\log p_\ell+1)^2-\dfrac{2}{n}\sum_{j=1}^{k-1}\sum_{\ell=j+1}^{k}p_\ell p_j(\log p_\ell+1)(\log p_j+1).
\end{equation}
where $n$ is the sample size. From this variance, we derive confidence intervals for entropy, which are used to assess uncertainty in the entropy-complexity plane. The asymptotic distribution of statistical complexity under the Multinomial law is:
\begin{equation}
		C[\widehat{\bm{p}}]=H[\widehat{\bm{p}}]Q[\widehat{\bm{p}}].
\end{equation}

%As defined by Rey et.al.~\cite{Rey2025} the asymptotic distribution of $C[\widehat{\bm{p}}]$ by a normal law, with mean of Shannon entropy ($\mu_C$) and variance (${\sigma}^2_C$)is given by:
%\begin{equation}
%	\mu_C=\dfrac{\max(Q'){\sigma_Q}{\sigma_p}}{n({\delta_Q}{\delta_H}+\rho)},
%\end{equation} 
%\begin{equation}
%	{\sigma}^2_C=\dfrac{{\max(Q')}^2{\sigma^2_Q}{\sigma^2_p}}{{n^2}({\delta^2_Q}+{\delta^2_H}+2\rho \delta_Q \delta_H+1+{\rho}^2)},
%\end{equation}
%where $\rho$ is the correlation coefficient bet ween normalized Shannon entropy and normalized disequilibrium.
%Further, 

%$\delta_Q=\dfrac{n\times \text{asymptotic mean of the Jensen Shannon divergence}(\mu_Q)}{\text{asymptotic variance of disequilibrium}(\sigma_Q)}$,

%$\delta_H=\dfrac{n\times \text{asymptotic mean of the Shannon Entropy}(\mu_C)}{\text{asymptotic variance of Shannon entropy}(\sigma_p)}$.

%Assuming that $n$ is sufficiently large, $\delta_Q$ and $\delta_H$ tend to infinity, we can calculate asymptotic distribution of the complexity.

This approach will be further analyzed, as described in the following objectives, to evaluate the accuracy of the results.

This proposal has three objectives in order to continue this research work.
\begin{itemize}
	\item Define a data base of time series for clustering, i.e., finding similar time series. 
	\item Extract all the features we know from their Bandt \& Pompe symbolization (Shannon, Tsallis and Renyi entropies, Fisher information measure, complexities, and the available confidence intervals)
	\item Use those features for time series clustering 
\end{itemize} 

