\chapter{The Research Project}\label{C:aim}

In this chapter, we outline the main ideas and objectives of this research project. 
%%% ACF Only complexity?
Section~\ref{Sec:AdvantagesLimitations} discusses entropy and complexity analysis in time series, highlighting its advantages and limitations. 
%%% ACF Use numbered sections/subsections and refer to their automatic numbers
Section~\ref{Sec:BP method} examines the advantages and  limitations of the Bandt and Pompe method. 
Section~\ref{Sec:BackgroundKnowledge}  provides background knowledge on the 
%%% ACF What is this plane?
entropy-complexity plane. 
Section~\ref{Sec:EntropyComplexity} explores the entropy-complexity plane for a broad class of time series. Section~\ref{Sec:AsymptoticDistribution} provides the asymptotic distribution of the entropy. Finally, the chapter concludes with the objectives of the research project and a case study related to our work.

%%% ACF Do you analyse complexity only?
\section{Entropy and Complexity Analysis in Time Series: Advantages and Limitations}\label{Sec:AdvantagesLimitations}

Entropy and complexity analysis provides powerful tools for characterizing the unpredictability and structural richness of dynamical systems, which evolve over time. Entropy measures, such as Shannon entropy (quantifying uncertainty in a probability distribution) and permutation entropy (which measures the order structure of a time series through ordinal patterns), are widely used to assess randomness and disorder. The main distinction is that permutation entropy computes Shannon entropy on the ordinal patterns extracted from a time series data.
%%% ACF What's the difference between SE and PE?
 

Complexity measures complement entropy by evaluating the balance between order and chaos. Together, entropy and complexity are particularly effective for detecting nonlinear patterns, relationships captured by nonlinear models, where inputs and outputs are not proportional and small changes can produce large or unpredictable effects. Such behavior frequently appears in biological, financial, or climate systems.

%%% ACF What is a nonlinear pattern?

While these methods reveal hidden structures and irregular dynamics beyond the reach of traditional linear approaches, they also require careful preprocessing, appropriate parameter selection (e.g., embedding dimension and time delay), and sufficient domain knowledge. Without this, interpretations may be misleading.


Despite these challenges, entropy and complexity remain essential in modern time series analysis. Unlike linear techniques, such as auto-correlation, regression, or Fourier analysis, that assume proportional and stationary relationships, entropy and complexity measures are designed to capture irregularities, pattern changes, and chaotic dynamics. This makes them invaluable for studying complex and nonlinear systems where conventional tools often fall short.
%%% ACF Several problems with the following sentence. (1) at some point, we will say that OPs are little sensitve to outliers, (2) we don't know what "data length" means
%They can be sensitive to noise and data length. 
%%% ACF Rethink the following assertion. I tend to disagree. Computing these quantities is not computationally intensive; but computing their statistical properties taking into account the serial correlation is
%%% ACF What is a high dimensional system?
%%% ACF What is a multi-scale system?


\section{The Bandt and Pompe Method: A Robust Approach} \label{Sec:BP method}

%%% ACF Reserve "demonstrate" for mathetical proofs
The concept of ordinal patterns in time series can be effectively studied through real world examples. 
Traditionally, numerous algorithms, techniques, and heuristics have been employed to estimate complexity measures from real world data. 
%%% ACF What is a low-dimensional dynamical system? the number of state variables or dimensions are small, we called it as low dimension. 1D, 2D or 3D can be considered as low dimension. 

However, these methods often perform well only for low-dimensional dynamical systems and struggle when noise is introduced. Low-dimensional dynamical systems are systems whose behavior can be described using a small number of variables or equations, typically two or three, such as the logistic map, or pendulum. These systems exhibit rich and often chaotic dynamics but remain mathematically tractable and easier to analyze using entropy and complexity measures. Because of their limited dimensionality, the patterns within the data are more distinct, making it easier to extract meaningful information. 

%%% ACF What is the Lyapunov exponent? It is one of the complexity parameters. It measures how small changes grow over time. 
The Bandt and Pompe method overcomes this limitation by providing a robust approach that remains reliable even in noisy environments. 
In time series analysis, key complexity parameters such as entropy, fractal dimension, 
%%% ACF If you mention it, you must  be able to define it
and Lyapunov exponents play a crucial role in comparing neighboring values and uncovering the underlying structure and dynamics of the data. A Lyapunov exponent measures the average rate at which nearby trajectories in a dynamical system diverge or converge. It provides deeper understanding of system's behavior. 

The advantages of Bandt \& Pompe methods:
\begin{itemize}
	\item Simplicity
	\item Extremely fast calculation
	\item Robustness
	\item Invariance to nonlinear monotonous transformations
\end{itemize}	

This method exhibits low sensitivity to noise and naturally accounts for the causal order of elements in a time series. As a result, it can be applied to various real-world problems, particularly in differentiating between chaotic and stochastic signals.

Despite its limitations, researchers have developed extensions to the original method to address its shortcomings and enhance its applicability to a broader range of complex systems.

\section{Statistical Complexity measures} \label{Sec:BackgroundKnowledge}

Bandt and Pompe introduced a highly effective method for analyzing time series within this framework. They calculated Shannon entropy based on the histogram of causal patterns and successfully identified chaotic components in sequences of words, among other applications.

Later, Rosso et al.~\cite{EEGAnalysisUsingWaveletBasedInformationTools} expanded this analysis by introducing an additional dimension: the statistical complexity derived from the same histogram of causal patterns. The authors have contributed to a wide range of applications. This approach, which utilizes the entropy-complexity plane, has been successfully applied to the visualization and characterization of different dynamical regimes as system parameters change  \cite{Bandt2005,Cao2004,DeMicco2012a,Kowalski2007,Kowalski2011b,Rosso2010a,Zunino2010a,Zunino2012a}, as well as to optical chaos \cite{Liu2016f,Soriano2011a,Toomey2014,Yang2015e,Zunino2011a}, hydrology \cite{Lange2013,Serinaldi2014,Stosic2016}, geophysics \cite{Consolini2014,Saco2010,Sippel2016}, engineering \cite{Aquino2017,Aquino2015,Redelico2017a,Yan2012}, biometrics \cite{Rosso2016}, characterization of pseudo-random number generators \cite{DeMicco2008,DeMicco2009}, biomedical signal analysis \cite{Li2014b,Li2008b,Li2007,Liang2015b,Montani2015a,Montani2014,Montani2014a,Montani2015,Morabito2012,Parlitz2012,Perinelli2025,Perinelli2019,Zanin2012}, and econophysics \cite{Bariviera2015a,Bariviera2015,Bariviera2016,Zanin2012,Zunino2016a,Zunino2009,Zunino2010}, to name a few.

After computing all symbols as described in Chapter~\ref{C:intro}, the histogram proportions are used to estimate the probability distribution of ordinal patterns. 
From this distribution, two key descriptors are calculated to characterize the time series:
\begin{enumerate}
	\item Entropy 
	
	\item Statistical complexity
\end{enumerate}
The most common metric for the first descriptor is the normalized Shannon entropy, defined as:
\begin{equation}
	H(\mathbf{p})=-\dfrac{1}{\log k}\sum^{k}_{\ell=1}p_{\ell} \ln{p_{\ell}}.
\end{equation}
Here, $k=D!$ represents the total number of possible permutation patterns.
%%% ACF Something is missing here
This entropy is bounded within the unit interval:
\begin{itemize}
	\item It reaches its minimum value $(H=0)$ when a single pattern dominates, i.e., $p_{\ell}=1$ for some $\ell$.
	\item It achieves its maximum $(H=1)$ under uniform probability $p_{\ell}=1/k$ for all $\ell$. 
\end{itemize}
This normalized entropy is often termed permutation entropy in time series analysis. 

While normalized Shannon entropy is a powerful tool for quantifying disorder, it fails to fully characterize complex dynamics. To address this limitation, López-Ruiz et al.~\cite{lopez1995statistical} introduced the disequilibrium $Q$ concept, which quantifies the deviation of a probability distribution $\mathbf{p}$ from a uniform (non-informative) equilibrium state. 
%%% ACF Here you say that the disequilibrium is the Euclidean distance, but then you say that it is the Jensen-Shannon
López-Ruiz and the team employed the Euclidean distance between $\mathbf{p}$ and the uniform distribution, providing a complementary metric to Shannon entropy for assessing structural complexity in systems.

%%% ACF Present the statistical complexity in a logical manner
The Jensen-Shannon distance between histogram of proportion $\mathbf{p}$ and the uniform probability function $\mathbf{u}=(1/k, 1/k, \dots, 1/k)$, where $k=D!$ corresponds to the number of possible permutation patterns provides a robust metric for quantifying deviations from uniformity. This distance measure, derived from the symmetric Jensen-Shannon divergence, is particularly suited for analyzing ordinal pattern distributions due to its ability to capture both structural differences and statistical disequilibrium in time series data.
%%% ACF We don't write like this: Here is the formula for $Q'$:
It is defined as:
\begin{equation}
	Q'(\mathbf{p,u})=\sum^k_{\ell=1} p_\ell\log\dfrac{p_\ell}{u_\ell}+u_\ell\log\dfrac{u_\ell}{p_\ell}.
\end{equation}

Lamberti et al. \cite{lamberti2004intensive} proposed Jensen-Shannon distance as a symmetric metric rooted in the Jensen-Shannon divergence. As the reference model, most works consider the uniform distribution $\mathbf{u}=(1/k,1/k, \dots, 1/k)$. The normalized disequilibrium is defined as follows
\begin{equation}
	Q=\dfrac{Q'}{\max{(Q')}},
\end{equation}
where $\max(Q')$ is defined as follows
\begin{equation}
	\max(Q')=-2 \left[\dfrac{k+1}{k}\log(k+1)-2\log(2k)+\log k\right].
\end{equation}

With this, Lamberti et al. \cite{lamberti2004intensive} proposed complexity as a measure of the statistical complexity of the underlying dynamics, which is defined as 
\begin{equation}
	C=HQ,
\end{equation}
where both $H$ and $Q$ are normalized quantities, therefore $C$ is also normalized. 

\section{The Entropy Complexity Plane} \label{Sec:EntropyComplexity}
The entropy-complexity plane is a two-dimensional representation where time series are mapped based on their entropy and statistical complexity. These metrics are derived from ordinal pattern distributions obtained through embedding dimension $D$ that are mapped on histograms of $D!$ bins. 

\subsection{Key Dynamics in the plane}
\begin{enumerate}
	\item Highly Ordered Systems, where the behavior is very predictable, structured, and often repeats in a regular pattern over time.
	
	Example: Strictly monotonic time series.
	\begin{itemize}
		\item Produces a single ordinal pattern $(H=0)$.
		
		\item Maximal disequilibrium (distance from uniform distribution).
		
		\item Maps to $(0,0),$ indicating minimal complexity.
	\end{itemize}
	
	\item Perfectly Random Systems
	
	Example: White noise
	\begin{itemize}
		\item Uniform ordinal pattern distribution $(H=1)$.
		\item Disequilibrium vanishes (distance $=0)$.
		\item Maps to $(1,0),$ reflecting maximal entropy without structural complexity.
	\end{itemize}
\end{enumerate}
The two extreme values are proved by Anteneodo \& Plastino \cite{anteneodo1996some}.
Expressions for the boundaries, derived using geometrical arguments within space configurations, were proposed by Martin et al. \cite{Martin2006}. 
These formulations provide a structured approach to understanding and analyzing the spatial behavior of specific systems or models. The lower boundary is characterized by a smooth curve, whereas the upper boundary consists of $D!-1$ distinct segments. As the embedding dimension $D$ approaches infinity, the upper boundary gradually converges into a smooth curve. 
Example for the entropy complexity plane is shown in Figure \ref{fig:complexity}. Further, ten time series and their points in the $H \times C$ plane for embedding dimension 6 according to our application is shown in Figure \ref{fig:tentimeseries}.

%%% ACF Illustrate the boundaries for D=3,4,5,6

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{complexity plane}
	\caption{Entropy Complexity Plane for Embedding dimension 3, 4,5, and 6}
	\label{fig:complexity}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{combined plot}
	\caption{Time series plots and their points in the $H \times C$ plane for Embedding dimension 6}
	\label{fig:tentimeseries}
\end{figure}

\section{Asymptotic Distribution of the permutation entropy}\label{Sec:AsymptoticDistribution}
Ordinal patterns are a robust symbolic transformation method that enables the unveiling of latent dynamics in time series data. This approach involves constructing histograms of patterns and calculating both entropy and statistical complexity. According to the literature, determining the exact distribution of features derived from ordinal patterns is challenging; therefore, researchers have extensively investigated this distribution and its related statistical properties. Two types of statistical distributions are discussed in the literature: the empirical distribution and the asymptotic distribution.

The empirical distribution is used in conjunction with knowledge of the expected variability of entropy and complexity, allowing hypothesis tests to be performed for a wide variety of models according to the underlying dynamics. Results in this direction can be found in the literature~\cite{Chagas2022a, DeMicco2008, Larrondo2005, Larrondo2006}. In this approach, researchers construct empirical distributions directly from observed data, which reflect the actual frequencies of patterns or outcomes.

Furthermore, two types of asymptotic distributions are discussed: the first assumes that patterns are independent and identically distributed (multinomial model), while the second accounts for dependent patterns.

\subsection {Asymptotic Distribution of the Shannon Entropy under the Multinomial Model}\label{Subsec:Multinomial} 

The multinomial distribution models counts of observations in $k$ mutually exclusive categories $\pi^1,\pi^2,\dots,\pi^k$ from $n$ independent trials, with probability vector $\mathbf{p} = (p_1, p_2, \dots, p_k)$, $p_\ell \ge 0$, $\sum_{\ell=1}^k p_\ell = 1$. Let $\mathbf{N} = (N_1, \dots, N_k)$ denote category counts, $\sum_{\ell=1}^k N_\ell = n$. Its probability mass function is
\[
\Pr(\mathbf{N} = \mathbf{n}) = n! \prod_{\ell=1}^k \frac{p_\ell^{n_\ell}}{n_\ell!}, \quad \mathbf{N} \sim \mathrm{Mult}(n, \mathbf{p}),
\]
with moments
\[
E(N_\ell) = n p_\ell, \quad
\mathrm{Var}(N_\ell) = n p_\ell (1-p_\ell), \quad
\mathrm{Cov}(N_\ell, N_j) = -n p_\ell p_j.
\]

The maximum likelihood (ML) estimator $\widehat{p}_\ell = N_\ell / n$ satisfies $n\widehat{\mathbf{p}} \sim \mathrm{Mult}(n,\mathbf{p})$. For any smooth function $g(\mathbf{p})$, the plug-in estimator $\widehat{g}(\mathbf{p}) = g(\widehat{\mathbf{p}})$ is also ML, enabling the asymptotic distribution of Shannon entropy
\begin{equation}
	H(\mathbf{p}) = -\sum_{\ell=1}^k p_\ell \log p_\ell,
	\label{eq:AsymEntropy}
\end{equation}
to be derived from the asymptotic normality of $\widehat{\mathbf{p}}$ as $n \to \infty$.
The Shannon's entropy of a multinomial distributed random variable is bounded between 0 and $\ln k$. The minimum is attained when $p_{\ell}=1$ for some $1\leq \ell \leq k$ and $p_j=0$ for every $j\ne \ell$. The expression is maximized by $p_{\ell}=1/k$ for every $1\leq \ell \leq k$.  

Further, asymptotic distribution can be described as follows. Let $X_n=(X_{1n},X_{2n},\dots,X_{kn})$ be a sequence of independent and identical distributed random vectors, with distribution $\text{Mult}(n,\mathbf{p})$. If $\widehat{\mathbf{p}}$ denotes the vector of sample proportions, and 
\[\mathbf{Y}_n=\sqrt{n}(\widehat{\mathbf{p}}-\mathbf{p})\] 
then
\[E(\mathbf{Y}_n)=\bm 0,\]
\[\text{Cov}(\mathbf{Y}_n)=\mathbf{D_p}-\mathbf{pp}^T,\]
where $\mathbf{D_p}=\text{Diag}(p_1,p_2,\dots,p_k)$, and the superscript $T$ denotes transposition. The asymptotic distribution of $\mathbf{Y}_n$ is multivariate normal with mean vector $\bm 0$ and covariance matrix $\mathbf{D_p}-\mathbf{pp}^T$, denoted as
%%% ACF Check how I wrote it
%The notation for this can be shown as follows.
%
\begin{equation}
	\mathbf{Y}_n \xrightarrow{\mathscr{D}} N(\mathbf{0}, \mathbf{D_p}-\mathbf{pp}^T).
	\label{eq:Cov} 
\end{equation}
 
Our focus is on the statistical properties of $H(\mathbf{p})$ when $\mathbf{p}$ is replaced by its maximum likelihood estimate $\widehat{\mathbf{p}} = (\widehat{p}_1, \widehat{p}_2, \dots, \widehat{p}_k)$. The problem thus reduces to determining the distribution of $H(\widehat{\mathbf{p}})$.

\begin{align}
	H(\widehat{\mathbf{p}})
	&= -\sum_{\ell=1}^k \widehat{p}_\ell \log \widehat{p}_\ell \nonumber \\
	&= -\sum_{\ell=1}^k \frac{N_\ell}{n} \log \frac{N_\ell}{n} \nonumber \\
	&= \log n - \frac{1}{n} \sum_{\ell=1}^k N_\ell \log N_\ell,
\end{align}
under $N=(N_1,N_2,\dots,N_k)\sim \text{Mult}(n,\mathbf{p})$

For the asymptotic distribution case, we refer to the Delta Method theorems and their multivariate version.

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\begin{theorem}[Delta Method, univariate]
	Let $X_n$ be a sequence of independent and identically distributed random variables such that 
	$\sqrt{n}(X_n - \theta) \xrightarrow{\mathscr{D}} N(0,\sigma^2)$. 
	Consider a function $h$ such that $h'(\theta)$ exists and $h'(\theta)\neq 0$. 
	Then,
	\[
	\sqrt{n}\,[h(X_n)-h(\theta)] \xrightarrow{\mathscr{D}} N\!\left(0,\,\sigma^2 [h'(\theta)]^2\right).
	\]
\end{theorem}

\begin{theorem}[Delta Method, multivariate]
	Let $\mathbf{X}_n= (X_{1n}, X_{2n}, \dots, X_{kn})$ be a sequence of independent and identically distributed random vectors such that 
	\[
	\sqrt{n}\,(\mathbf{X}_n - \boldsymbol{\theta}) \xrightarrow{\mathscr{D}} N_k(\mathbf{0},\Sigma),
	\]
	where $\boldsymbol{\theta} = (\theta_1,\theta_2,\dots,\theta_k)$ and $\Sigma$ is the covariance matrix. 
	Suppose that $h\colon\mathbb{R}^k \to \mathbb{R}^m$ is continuously differentiable in a neighborhood of $\boldsymbol{\theta}$, with Jacobian matrix
	\[
	B = \left( \frac{\partial h_i}{\partial \theta_j} \right)_{i,j=1}^k,
	\]
	which is non-singular at $\boldsymbol{\theta}$. Then,
	\[
	\sqrt{n}\,\big(h(\mathbf{X}_n)-h(\boldsymbol{\theta})\big) \xrightarrow{\mathscr{D}} N_m\!\left(\mathbf{0},\, B \Sigma B^T\right).
	\]
\end{theorem}

Further, covariance matrix of Equation~\eqref{eq:Cov} can be expressed as 
%%% ACF Use \eqref when referring to equations
%%% ACF Do not leave blank lines between elements of the same sentence
\begin{equation}
	(\mathbf{D_p}-\mathbf{pp}^T)_{\ell j} =
	\begin{cases}
		p_{\ell}(1-p_{\ell}), & \text{if } \ell = j, \\[6pt]
		-\,p_{\ell}p_{j}, & \text{if } \ell \neq j,
	\end{cases}
\end{equation}
for $1\leq {\ell}, j\leq k.$
Even for dependent processes (e.g., Markov chains), normalized Shannon entropy converges to a normal distribution with variance determined by the covariance structure of $\widehat{\mathbf{p}}$. 

Rey et. al.~\cite{Rey2025} derived the asymptotic distribution of statistical complexity, defined as normalized Shannon entropy times normalized Jensen–Shannon divergence from the uniform distribution under the multinomial model. They demonstrated convergence to normality, with variance and bias reflecting the system’s dynamics. Numerical studies confirm robustness even when the multinomial model is approximate, such as in Bandt–Pompe ordinal patterns. 

We refer the asymptotic equation for the mean and variance provided by Rey et. al.~\cite{Rey2025} for our research work.
%%% ACF The following is strange; the left part depends on p, the right-hand side depends on \widehat p: Rasika corrected it
\begin{equation}
	\mu_{n,\widehat{\mathbf{p}}}	= H(\widehat{\mathbf{p}})
	= -\sum_{\ell=1}^k \widehat{p}_\ell \log \widehat{p}_\ell 
	\label{eq:AsympMean}
\end{equation}  

%%% ACF Is the verb missing?
The estimator in~\eqref{eq:AsympMean} is normally distributed with mean $H(\mathbf{p})$ and variance $\sigma^2_{n,\mathbf{p}}$, where
\begin{equation}
	\sigma^2_{n,\mathbf{p}}=\dfrac{1}{n}\sum_{\ell=1}^{k}p_\ell(1-p_\ell)(\log p_\ell+1)^2-\dfrac{2}{n}\sum_{j=1}^{k-1}\sum_{\ell=j+1}^{k}p_\ell p_j(\log p_\ell+1)(\log p_j+1).
\end{equation}


\subsection{Asymptotic distribution of Permutation Entropy under the pattern dependence}\label{Subsec:PatternDependence}

As we discussed earlier, real-valued time series $\mathbf{x}=\{x_1,x_2,\dots,x_{n+D-1}\}$  transform into the series symbols $\mathbf{{\pi}}=({\pi}_1, {\pi}_2,\dots, {\pi}_n)$ from sub-sequences of embedding dimension $D$, where we considered $D!=k$. Due to the overlapping of time windows, the ordinal patterns which we calculate are dependent. 
%%% ACF Use \dots and correct punctuation
For $i=1,2,\dots k$, let $p_i$ be the probability of observing the state $\pi_i$, denote the vector probabilities, $\mathbf{p}={\left\{p_1,p_2,\dots,p_k\right\}}$ and express as $\mathbf{D_p}=\text{Diag}(p_1,p_2, \dots, p_k)$ the diagonal matrix. 
The transition probability of reaching state $\pi_j$ at time 
%%% ACF Is this the same \ell as before? No, I corrected it.
$t+r$ from the state $\pi_i$ at time $t$, for $r=1,2,\dots,D-1$, is denoted by $p^{(r)}_{ij}$. These transition probabilities can be collected in the matrix $\mathbf{Q}^{(r)}$ whose elements are  $p^{(r)}_{ij}$. As describe in the Rey et al.~\cite{Rey2023a} when $n$ is sufficiently large, asymptotic variance of the Shannon entropy defined as follows:

\begin{equation}
	\begin{split}
		\widehat{\nu}^2_{\widehat{\mathbf{p}}} = \widehat{\sigma}^2_{\widehat{\mathbf{p}}} + 
		& \sum_{i=1}^{k}(\log p_i + 1)^2 
		\left[ (2D - 2)p_i^2 + 2\sum_{r=1}^{D-1} \mathbf{Q}^{(r)}_{ii} \right] \\
		& - 2 \sum_{i=1}^{k-1} \sum_{j=i+1}^{k} (\log p_i + 1)(\log p_j + 1) \\
		& \quad \times \left[ (2D - 1)p_i p_j - \sum_{r=1}^{D-1} \left( \mathbf{Q}^{(r)}_{ij} + \mathbf{Q}^{(r)}_{ji} \right) \right].
	\end{split}
	\label{eq:asympvar}
\end{equation} 


%%% ACF The first sentence repeats the last of the previous paragraph: remove the preious content 
%Asymptotic variance of the Shannon entropy of ordinal patterns considering their correlation structure is given in Equation~\ref{eq:asympvar}.
%%% ACF Again, a deterministic quantity does not have a normal distribution: Modified the sentence
For practical purposes, given $n$ sufficiently large, the estimator in~\eqref{eq:AsymEntropy} can be approximated by a normal distribution with mean $H(\widehat{\mathbf{p}})$ and variance $\widehat{\nu}^2_{\widehat{\mathbf{p}}}/n$. In addition, for $\alpha \in (0,1)$ and sufficiently large $n$, the $(1-\alpha)$\SI{100}{\percent} confidence interval for the estimated entropy is given below.

\begin{equation}
  H(\widehat{\mathbf{p}})\pm \dfrac{Z_{\alpha/2}\widehat{\nu}_{\widehat{\mathbf{p}}}}{\sqrt{n}},
  \label{eq:ConfidenceInterval}
\end{equation} 
where $Z_{\alpha/2}$ is the $\alpha/2$-quantile of a standard normal random variable.

For convenience, Equation~\ref{eq:ConfidenceInterval} is referred to and defined as follows.
\begin{equation}
	H(\widehat{\mathbf{p}})\pm \textbf{Semi Length},
	\label{eq:CI}
\end{equation} 
where $\textbf{Semi Length}=\dfrac{Z_{\alpha/2}\widehat{\nu}_{\widehat{\mathbf{p}}}}{\sqrt{n}}$

\subsection{Asymptotic Variance of Statistical Complexity $C(\widehat{\mathbf{p}})$} \label{Subsec:AsympVarComplexity} 
The asymptotic variance of the statistical complexity $C(\widehat{\mathbf{p}})$, under both the multinomial model and dependence, can be expressed using formulas derived for permutation entropy, the Jensen-Shannon divergence, and their joint asymptotics. The key approach follows Rey et al.~\cite{Rey2025} and recent research work by Silbernagel et al.~\cite{silbernagel2025joint} , employing the multivariate Delta method for functions of multinomial proportions.


%%% ACF If it is estimated, it is a constant and its variance is zero: corrected it. 
The variance of the complexity, $C(\widehat{\mathbf{p}})$, has two estimators:
\begin{itemize}
	%%% ACF Provide the expression taking care to make the notation match previous definitions
	\item \textbf{Multinomial model variance}, denoted as $\widehat{\omega}^2_{\widehat{\mathbf{p}}}$, which assumes independent ordinal patterns.
	\item \textbf{Serial dependence variance}, denoted as $\widehat{\eta}^2_{\widehat{\mathbf{p}}}$.
\end{itemize}

\subsubsection{Formula for Asymptotic Variance of Statistical Complexity}
Let:
\begin{itemize}
	\item $\mathbf{p} = (p_1,\dots, p_k):$ ordinal patterns probability vector
	\item $\widehat{\mathbf{p}} = (\widehat{p}_1,\dots, \widehat{p}_k):$ empirical probability vector
	\item $n:$ sample size
	\item $H(\widehat{\mathbf{p}}):$ normalized Shannon entropy
	\item $Q(\widehat{\mathbf{p}}):$ normalized Jensen Shannon divergence from the uniform distribution
	\item $C(\widehat{\mathbf{p}})=H(\widehat{\mathbf{p}}) \times Q(\widehat{\mathbf{p}}):$ normalized statistical complexity
\end{itemize}

\begin{enumerate}
	\item \textbf{Multinomial Model (Independence)}

The estimator is asymptotically normal:
$$ \sqrt{n}(C(\widehat{\mathbf{p}})-C(\mathbf{p})) \xrightarrow{\mathscr{D}} N(\mathbf{0}, \widehat{\omega}^2_{\widehat{\mathbf{p}}}),$$
where the asymptotic variance is 
$$\widehat{\omega}^2_{\widehat{\mathbf{p}}}= \nabla C(\mathbf{p})^T \Sigma \nabla C(\mathbf{p}).$$

\begin{itemize}
	\item $\Sigma =\mathbf{D_p}-\mathbf{p}\mathbf{p}^T$ is the covariance matrix of sample proportions under independence. 
	\item $\nabla C(\mathbf{p})$ is the gradient (vector of partial derivatives) of $C$ with respect to $\mathbf{p}:$
	$$\dfrac{\partial C}{\partial p_\ell}=Q(\mathbf{p})\dfrac{\partial H}{\partial p_\ell}+H(\mathbf{p})\dfrac{\partial Q}{\partial p_\ell},$$
	where: 
	
	\item $\dfrac{\partial H}{\partial p_\ell}=-(\log p_\ell +1),$ 
	\item The exact partial derivative of Jensen-Shannon divergence $Q(\mathbf{p})$ with respect to depends on the definition but can be explicitly calculated.
\end{itemize}


	\item \textbf{Serial Dependence Case} 

For dependent ordinal patterns, the asymptotic variance increases: 
$$\widehat{\eta}^2_{\widehat{\mathbf{p}}} = a \times \widehat{\omega}^2_{\widehat{\mathbf{p}}},$$

where:
\begin{itemize}
	\item 
	$a = \dfrac{\widehat{\nu}^2_{\widehat{\mathbf{p}}}}{\widehat{\sigma}^2_{\widehat{\mathbf{p}}}},$ 
	\item  $\widehat{\nu}^2_{\widehat{\mathbf{p}}}$ is the variance of entropy under serial dependence,
	\item $\widehat{\sigma}^2_{\widehat{\mathbf{p}}}$ is the variance of entropy under independence.
\end{itemize}

%%% ACF Provide the equations
% \item \textbf{Variance Expression for Complexity (Expanded)}
\end{enumerate}
 Bringing these results together:
\[ \text{Var}(C(\widehat{\mathbf{p}})) = \widehat{\eta}^2_{\widehat{\mathbf{p}}} \approx a \nabla C(\mathbf{{p}})^T \Sigma \nabla C(\mathbf{{p}})
\]

\subsection{Other types of Entropy}
%%% ACF At the end of this section, what do we know about them? Any asymptotic result? Under which conditions?

Moreover, other types of descriptors, such as Rényi entropy~\cite{renyi1961measures}, Tsallis entropy~\cite{tsallis1988possible}, and Fisher information~\cite{frieden2004science}, have been proposed to extract additional information that is not captured by Shannon entropy.
From these entropy measures, Fisher information has garnered more attention due to its unique properties. Fisher information is defined as the average logarithmic derivative of a continuous probability density function.

For discrete probability distributions, Fisher information can be approximated by calculating the differences between probabilities of consecutive distribution elements. A key distinction between Shannon entropy and Fisher information lies in their focus: Shannon entropy quantifies the overall unpredictability of a system, while Fisher information measures the rate of change between consecutive observations, making it more sensitive to small changes and perturbations.

The following equations define Tsallis entropy $	(H_{T}^{q}(\widehat{\mathbf{p}}))$, Rényi entropy $(H_{R}^{q}(\widehat{\mathbf{p}}))$, and Fisher information measures $(H_{F}(\widehat{\mathbf{p}}))$ \cite{sanchez2009discrete} :
\begin{equation}
	H_{T}^{q}(\widehat{\mathbf{p}})=\sum_{\ell=1}^{k}\dfrac{\widehat{p_\ell}-\widehat{p_\ell}^q}{q-1},
\end{equation}
where the index $q\in \mathbb{R}\backslash \{1\}$
\begin{equation}
	H_{R}^{q}(\widehat{\mathbf{p}})=\dfrac{1}{1-q} \log \sum_{\ell=1}^{k}{\widehat{p_\ell}}^q,
\end{equation}
where the index $q\in \mathbb{R}^{+}\backslash \{1\}$
\begin{equation}
	H_F(\widehat{\mathbf{p}})=F_0\sum_{\ell=1}^{k-1}\Big(\sqrt{\widehat{p_\ell}_{+1}}-\sqrt{\widehat{p_\ell}}\Big)^2 ,
\end{equation}
where the re-normalization coefficient is $F_0=4$ \cite{sanchez2009discrete}

Rényi entropy, Tsallis entropy, and Fisher information are alternative statistical measures that reveal specific aspects of time-series structure and variability not captured by Shannon entropy. Rey et al. demonstrated that, when computed from empirical ordinal pattern distributions, these measures become increasingly reliable as sample size grows. Specifically, their sample estimates approach well-defined asymptotic distributions that can be used for statistical inference and hypothesis testing.

For the multinomial model, where ordinal patterns are assumed independent and drawn according to fixed probabilities, the sample entropies and Fisher information converge in distribution as $n$ increases: the central limit theorem applies, yielding asymptotic normality for Rényi and Tsallis entropy estimators, as well as for Fisher information. Explicit formulas for mean and variance are available, and confidence intervals can be constructed accordingly. If the empirical histogram is used, these asymptotic results remain valid under large sample size and mild regularity. In all cases, normality holds for entropy-type measures in the multinomial setting, and specific corrections or other distributions (like chi-squared for quadratic forms) may arise for more complex dependency structures. Thus, under the multinomial and empirical frameworks, these advanced entropy and information measures possess robust, predictable properties that enable their practical use in time-series analysis and statistical hypothesis testing.


\section{Case Study of Asymptotic Distribution of the Shannon Entropy under pattern dependence and independence} \label{Sec:CaseStudy} 

Statistical complexity is defined as the product of two normalized quantities:
\begin{itemize}
	\item The Shannon entropy,
	\item The Jensen-Shannon distance between the observed probability distribution and the uniform distribution. 
\end{itemize}

In this section we discuss two key aspects with real world scenario:
\begin{enumerate}
	\item \textbf{Significance of Asymptotic Distributions}: Why understanding large-sample behavior matters for statistical inference,
%	\item \textbf{Multinomial Model Framework}: Derivation of the asymptotic distribution for statistical complexity under Multinomial assumptions
	\item \textbf{Practical Formula}: A working equation for calculating the asymptotic distribution of complexity.
\end{enumerate}

As a case study for our work, we consider data from the Bearing Data Center and the seeded fault test data from Case Western Reserve University, School of Engineering. The datasets includes ball bearing test data for normal bearings as well as single-point defects on the fan end and drive end. Data were collected at a rate of $48,000 (48k$ drive-end) data points per second during bearing tests. Each file contains motor loads $(0, 1, 2,$ and $3)$, drive-end vibration data, and fan-end vibration data. The approximate motor speeds in RPM during testing: $1797, 1772, 1750,$ and $1730$. For our case study, we consider two time series (Normal Baseline and $48k$ Drive-End) with a motor load of $0$ and an RPM of $1797$. 

The primary objective of this study is to detect malfunctioning machinery by analyzing two time series using ordinal patterns. We introduce a distance metric based on the ordinal structure of the segments to quantify similarity. This metric facilitates the identification of faulty machines across various embedding dimensions, ranging from $3$ to $6$. For this case study, we employ an embedding dimension of $3$ for convenience; subsequent analyses will extend to the remaining dimensions to compare results. Permutation entropy under asymptotic conditions is computed by considering the probability distribution of ordinal patterns. The results are further analyzed using the complexity–entropy plane, providing insights into the system's dynamics.

Initially, we analyzed complete datasets from two time series: one comprising 250,000 data points representing the normal baseline at motor load 0, and another containing 2,540,000 data points from the 48k drive end under the same motor load. We computed the entropy and complexity measures for these entire datasets, followed by the calculation of the asymptotic variance as defined in Section~\ref{Subsec:PatternDependence}. This asymptotic variance with pattern independence was then used to determine the confidence interval for entropy (Equation is defined in~\ref{eq:ConfidenceInterval}). The calculation of the semi-length of the interval is given by Equation~\ref{eq:CI}. 
%%% ACF With or without correlation? Justify
The final results are presented in Table~\ref{tab:EnComplexResults}.
%%% ACF Why omitting the standard deviation of the statistical complexity in the table? I did not calculate it for this data set

%%% ACF What is each line?
%%% ACF $\sigma_{\bm{p}}$ is a new notation; moreover, is it an estimate?
\begin{table}[H]
	\centering
	\begin{tabular}{llcr}
		\toprule
		Entropy  & Complexity  & $\widehat{\sigma}_{\widehat{\mathbf{p}}}$ & Semi Length \\
		\midrule
		$0.665235$ & $0.226447$ & $0.358893$ & $0.000441$\\ 
		$0.772973$ & $0.170954$ & $0.324376$ & $0.001287$\\
		\bottomrule
	\end{tabular}
	\caption{Entropy Complexity Results}
	\label{tab:EnComplexResults}
\end{table}

Subsequently, we segmented the data into batches of $10,000$ points, categorizing them as either `Normal` or `48k Drive End`. We then performed a batch wise comparison of entropy and complexity metrics to identify fault data segments. 
The normal dataset comprises $25$ batches, all corresponding to motor load $0$, while the $48k$ drive end dataset includes $254$ batches. Due to the extensive volume of entropy and complexity data generated, the complete results table is not included in this report. However, the entropy–complexity plane effectively illustrates both batch-wise and full-data analyses. As depicted in Figure \ref{fig:EntopyComplexity Plane} below, faulty machines form a distinct cluster in the entropy–complexity plane, highlighting their deviation from normal operational patterns. 
It is clear from the graph that there are both overlapping and non-overlapping confidence intervals. This indicates that some machines differ significantly, while others do not. The main purpose of our experiment is to identify faulty machines. Therefore, we highly recommend extending these results by increasing the embedding dimension to better understand the final outcomes. The general framework of this experiment is also provided in this chapter to clarify the main objective of the research.

\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.8 \textwidth]{confidence_interval}
	\caption{Entropy Complexity Plane}
	\label{fig:EntopyComplexity Plane}
\end{figure}

In addition to this we analyze the full data results for higher embedding dimension $D=6$.
	\begin{figure}[hbt]
	\centering
	\includegraphics[width=0.8 \textwidth]{Confidence Interval}
	\caption{Entropy Complexity Plane for $D=6$}
	\label{fig:EntopyComplexity Plane D=6}
\end{figure}

Because the original case study involved a large sample size, we computed entropy and statistical complexity for smaller sample sizes of 100, 1000, and 2000. These values were then analyzed using both the Multinomial and Serial Dependence models. The analysis clearly demonstrates the confidence intervals for both entropy and complexity.	

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8 \textwidth]{CI for Multinomial model}
	\caption{Confidence Interval for Multinomial Model}
	\label{fig:CIMultinorm}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7 \textwidth]{CI for pattern dependence}
	\caption{Confidence Interval Pattern Dependence model}
	\label{fig:CIDependence}
\end{figure}

The general framework for analyzing entropy-complexity planes with confidence intervals are given as follows.
\begin{enumerate}
	\item \textbf{Calculate Entropy (H) and Complexity (C):} 
	appropriate estimator are Shannon entropy, statistical complexity measures 
	\item \textbf{Compute Confidence Intervals:}
	Generate multiple resampled datasets to estimate the variance of $H$ and $C$.
	\item \textbf{Plot on Entropy-Complexity Plane:}
	\begin{itemize}
		\item Axes: x-axis: Entropy $(H)$, y-axis: Statistical complexity $(C)$
		\item Data Points: Plot individual or aggregated results.
		\item Confidence Regions: Represent uncertainty 
	\end{itemize}
	
	\item \textbf{Interpretation}
	\begin{table}[H]
		\centering
		\begin{tabular}{cr}
			\toprule
			Region of Plane  & Interpretation  \\
			\midrule
			High $H$ and High $C$ & Complex, structured systems \\ 
			Low $H$ and Low $C$ & Simple, predictable systems \\
			High $H$ and Low $C$ & Random/noisy systems \\
			Low $H$ and High $C$ & Non-random systems \\ 
			\bottomrule
		\end{tabular}
	\end{table}
	
	\item \textbf{Statistical testing:}
	\begin{itemize}
		\item Compare confidence intervals between groups to assess significant differences.
		\item Overlapping intervals $\rightarrow$ {No significant difference}.
		\item Non-overlapping intervals $\rightarrow$ Potential significance.
	\end{itemize}
\end{enumerate}







